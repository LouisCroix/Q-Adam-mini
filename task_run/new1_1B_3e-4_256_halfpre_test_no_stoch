Starting script
Initializing wandb

pad_token_id: 32000

----------------------------------------
Prepare Model for Int8 Training
----------------------------------------
Model parameters dtype: torch.uint8

Memory allocated before model put to GPU: 0, Unit: MB, same below.
Memory allocated after model put to GPU: 1676
Memory ocupied by model: 1676
The otherwise_dtype is torch.float32, which is used for all tensors not quantized in this optimizer.
Adam-mini found the param block with name: model.embed_tokens.weight torch.Size([32001, 2048])
Adam-mini found the param block with name: model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.0.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.0.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.0.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.0.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.0.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.0.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.0.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.0.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.1.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.1.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.1.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.1.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.1.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.1.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.1.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.1.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.1.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.2.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.2.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.2.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.2.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.2.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.2.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.2.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.2.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.2.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.3.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.3.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.3.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.3.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.3.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.3.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.3.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.3.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.3.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.4.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.4.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.4.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.4.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.4.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.4.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.4.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.4.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.4.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.5.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.5.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.5.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.5.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.5.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.5.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.5.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.5.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.5.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.6.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.6.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.6.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.6.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.6.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.6.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.6.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.6.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.6.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.7.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.7.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.7.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.7.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.7.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.7.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.7.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.7.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.7.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.8.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.8.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.8.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.8.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.8.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.8.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.8.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.8.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.8.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.9.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.9.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.9.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.9.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.9.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.9.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.9.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.9.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.9.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.10.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.10.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.10.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.10.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.10.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.10.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.10.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.10.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.10.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.11.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.11.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.11.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.11.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.11.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.11.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.11.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.11.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.11.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.12.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.12.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.12.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.12.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.12.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.12.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.12.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.12.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.12.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.13.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.13.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.13.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.13.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.13.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.13.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.13.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.13.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.13.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.14.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.14.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.14.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.14.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.14.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.14.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.14.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.14.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.14.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.15.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.15.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.15.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.15.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.15.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.15.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.15.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.15.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.15.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.16.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.16.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.16.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.16.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.16.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.16.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.16.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.16.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.16.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.17.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.17.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.17.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.17.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.17.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.17.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.17.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.17.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.17.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.18.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.18.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.18.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.18.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.18.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.18.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.18.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.18.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.18.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.19.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.19.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.19.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.19.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.19.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.19.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.19.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.19.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.19.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.20.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.20.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.20.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.20.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.20.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.20.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.20.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.20.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.20.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.21.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.21.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.21.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.21.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.21.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.21.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.21.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.21.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.21.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.22.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.22.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.22.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.22.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.22.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.22.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.22.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.22.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.22.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.23.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.23.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.23.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.23.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.23.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.23.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.23.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.23.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.23.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.norm.weight torch.Size([2048])
Adam-mini found the param block with name: lm_head.weight torch.Size([32001, 2048])
Adam-mini found 1 embedding layers, 1 output layers; 48 Querys and Keys;  0 Values;  24 attn_proj;  72 MLPs;
=====>>>  Warning by Adam-mini: No Value found. If you are training Transformers, please check the name of your Value in attention blocks and manually add them to 'self.wv_names' of Adam-mini. You can do this by adding an additional lines of code: optimizer.wv_names.add('the keywords in the  name of your Value' ). 
Memory allocated: 4533
Memory reserved: 30212

Update step: 13. Tokens seen: 3670016
Training loss: 10.758404041712101
Learning rate: 1.9499999999999995e-06
Memory allocated: 6044
Memory reserved: 31256

Update step: 26. Tokens seen: 7077888
Training loss: 10.734804813678448
Learning rate: 3.899999999999999e-06
Memory allocated: 6044
Memory reserved: 31256

Update step: 39. Tokens seen: 10485760
Training loss: 10.682114536945637
Learning rate: 5.85e-06
Memory allocated: 6044
Memory reserved: 31256

Update step: 52. Tokens seen: 13893632
Training loss: 10.601486222102093
Learning rate: 7.799999999999998e-06
Memory allocated: 6044
Memory reserved: 31256

Update step: 65. Tokens seen: 17301504
Training loss: 10.474512226306475
Learning rate: 9.75e-06
Memory allocated: 6044
Memory reserved: 31256

Update step: 78. Tokens seen: 20709376
Training loss: 10.292921020434452
Learning rate: 1.17e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 91. Tokens seen: 24117248
Training loss: 10.011484439556416
Learning rate: 1.3649999999999998e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 104. Tokens seen: 27525120
Training loss: 9.599525437905239
Learning rate: 1.5599999999999996e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 117. Tokens seen: 30932992
Training loss: 9.071918673240221
Learning rate: 1.755e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 130. Tokens seen: 34340864
Training loss: 8.467328310012817
Learning rate: 1.95e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 143. Tokens seen: 37748736
Training loss: 7.966161980078771
Learning rate: 2.1449999999999996e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 156. Tokens seen: 41156608
Training loss: 7.652363049296232
Learning rate: 2.34e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 169. Tokens seen: 44564480
Training loss: 7.539560492222126
Learning rate: 2.535e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 182. Tokens seen: 47972352
Training loss: 7.524136182207328
Learning rate: 2.7299999999999996e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 195. Tokens seen: 51380224
Training loss: 7.464388641027304
Learning rate: 2.925e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 208. Tokens seen: 54788096
Training loss: 7.450670628593518
Learning rate: 3.119999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 221. Tokens seen: 58195968
Training loss: 7.423506676004483
Learning rate: 3.315e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 234. Tokens seen: 61603840
Training loss: 7.374190720228048
Learning rate: 3.51e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 247. Tokens seen: 65011712
Training loss: 7.355010259609956
Learning rate: 3.705e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 260. Tokens seen: 68419584
Training loss: 7.30607888675653
Learning rate: 3.9e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 273. Tokens seen: 71827456
Training loss: 7.251585600467829
Learning rate: 4.095e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 286. Tokens seen: 75235328
Training loss: 7.174471129591648
Learning rate: 4.289999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 299. Tokens seen: 78643200
Training loss: 7.114901228592946
Learning rate: 4.484999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 312. Tokens seen: 82051072
Training loss: 7.033535462159377
Learning rate: 4.68e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 325. Tokens seen: 85458944
Training loss: 6.954921425535129
Learning rate: 4.875e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 338. Tokens seen: 88866816
Training loss: 6.9364760873409415
Learning rate: 5.07e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 351. Tokens seen: 92274688
Training loss: 6.838375324240098
Learning rate: 5.264999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 364. Tokens seen: 95682560
Training loss: 6.809484349993559
Learning rate: 5.459999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 377. Tokens seen: 99090432
Training loss: 6.778370364354207
Learning rate: 5.654999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 390. Tokens seen: 102498304
Training loss: 6.751548012861838
Learning rate: 5.85e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 403. Tokens seen: 105906176
Training loss: 6.66322188767103
Learning rate: 6.045e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 416. Tokens seen: 109314048
Training loss: 6.610828168117083
Learning rate: 6.239999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 429. Tokens seen: 112721920
Training loss: 6.503746101489434
Learning rate: 6.434999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 442. Tokens seen: 116129792
Training loss: 6.451294905864275
Learning rate: 6.63e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 455. Tokens seen: 119537664
Training loss: 6.383022774870579
Learning rate: 6.824999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 468. Tokens seen: 122945536
Training loss: 6.363037750124931
Learning rate: 7.02e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 481. Tokens seen: 126353408
Training loss: 6.308197630139498
Learning rate: 7.214999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 494. Tokens seen: 129761280
Training loss: 6.271711244032933
Learning rate: 7.41e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 507. Tokens seen: 133169152
Training loss: 6.227446454075666
Learning rate: 7.604999999999999e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 520. Tokens seen: 136577024
Training loss: 6.191711329496824
Learning rate: 7.8e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 533. Tokens seen: 139984896
Training loss: 6.1572834975444355
Learning rate: 7.994999999999999e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 546. Tokens seen: 143392768
Training loss: 6.120484759028141
Learning rate: 8.19e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 559. Tokens seen: 146800640
Training loss: 6.0865291357040405
Learning rate: 8.385e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 572. Tokens seen: 150208512
Training loss: 6.050676330924034
Learning rate: 8.579999999999998e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 585. Tokens seen: 153616384
Training loss: 6.072067357026613
Learning rate: 8.774999999999999e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 598. Tokens seen: 157024256
Training loss: 5.999377121145908
Learning rate: 8.969999999999998e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 611. Tokens seen: 160432128
Training loss: 6.003429318849857
Learning rate: 9.164999999999999e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 624. Tokens seen: 163840000
Training loss: 5.958595795127062
Learning rate: 9.36e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 637. Tokens seen: 167247872
Training loss: 5.927539051725314
Learning rate: 9.554999999999999e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 650. Tokens seen: 170655744
Training loss: 5.896963472549732
Learning rate: 9.75e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 663. Tokens seen: 174063616
Training loss: 5.882520271035341
Learning rate: 9.944999999999999e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 676. Tokens seen: 177471488
Training loss: 5.960055966789906
Learning rate: 0.0001014
Memory allocated: 6044
Memory reserved: 32258

Update step: 689. Tokens seen: 180879360
Training loss: 5.830150045454502
Learning rate: 0.00010334999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 702. Tokens seen: 184287232
Training loss: 5.820654636392226
Learning rate: 0.00010529999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 715. Tokens seen: 187695104
Training loss: 5.774944007396698
Learning rate: 0.00010724999999999999
Memory allocated: 6044
Memory reserved: 32258

Update step: 728. Tokens seen: 191102976
Training loss: 5.803647457407071
Learning rate: 0.00010919999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 741. Tokens seen: 194510848
Training loss: 5.7891811568003435
Learning rate: 0.00011114999999999999
Memory allocated: 6044
Memory reserved: 32258

Update step: 754. Tokens seen: 197918720
Training loss: 5.731779930683283
Learning rate: 0.00011309999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 767. Tokens seen: 201326592
Training loss: 5.733324344341572
Learning rate: 0.00011504999999999999
Memory allocated: 6044
Memory reserved: 32258

Update step: 780. Tokens seen: 204734464
Training loss: 5.696364424549616
Learning rate: 0.000117
Memory allocated: 6044
Memory reserved: 32258

Update step: 793. Tokens seen: 208142336
Training loss: 5.70437741623475
Learning rate: 0.00011894999999999999
Memory allocated: 6044
Memory reserved: 32258

Update step: 806. Tokens seen: 211550208
Training loss: 5.682212081093055
Learning rate: 0.0001209
Memory allocated: 6044
Memory reserved: 32258

Update step: 819. Tokens seen: 214958080
Training loss: 5.666547653766779
Learning rate: 0.00012284999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 832. Tokens seen: 218365952
Training loss: 5.621831528269327
Learning rate: 0.00012479999999999997
Memory allocated: 6044
Memory reserved: 32258

Update step: 845. Tokens seen: 221773824
Training loss: 5.640357716725423
Learning rate: 0.00012675
Memory allocated: 6044
Memory reserved: 32258

Update step: 858. Tokens seen: 225181696
Training loss: 5.596954744595748
Learning rate: 0.00012869999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 871. Tokens seen: 228589568
Training loss: 5.5934965530267124
Learning rate: 0.00013064999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 884. Tokens seen: 231997440
Training loss: 5.586261023695652
Learning rate: 0.0001326
Memory allocated: 6044
Memory reserved: 32258

Update step: 897. Tokens seen: 235405312
Training loss: 5.589993754258523
Learning rate: 0.00013455
Memory allocated: 6044
Memory reserved: 32258

Update step: 910. Tokens seen: 238813184
Training loss: 5.528851997393828
Learning rate: 0.00013649999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 923. Tokens seen: 242221056
Training loss: 5.5312266911451635
Learning rate: 0.00013845
Memory allocated: 6044
Memory reserved: 32258

Update step: 936. Tokens seen: 245628928
Training loss: 5.4945880977007056
Learning rate: 0.0001404
Memory allocated: 6044
Memory reserved: 32258

Update step: 949. Tokens seen: 249036800
Training loss: 5.522827171362364
Learning rate: 0.00014235
Memory allocated: 6044
Memory reserved: 32258

Update step: 962. Tokens seen: 252444672
Training loss: 5.494421752599569
Learning rate: 0.00014429999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 975. Tokens seen: 255852544
Training loss: 5.468008739443926
Learning rate: 0.00014624999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 988. Tokens seen: 259260416
Training loss: 5.46413841843605
Learning rate: 0.0001482
Memory allocated: 6044
Memory reserved: 32258

Update step: 1001. Tokens seen: 262668288
Training loss: 5.463628680660174
Learning rate: 0.00015014999999999996
Memory allocated: 6044
Memory reserved: 32258

Update step: 1014. Tokens seen: 266076160
Training loss: 5.450818167282985
Learning rate: 0.00015209999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1027. Tokens seen: 269484032
Training loss: 5.437279076530383
Learning rate: 0.00015404999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1040. Tokens seen: 272891904
Training loss: 5.887120609100048
Learning rate: 0.000156
Memory allocated: 6044
Memory reserved: 32258

Update step: 1053. Tokens seen: 276299776
Training loss: 5.578901886940002
Learning rate: 0.00015794999999999996
Memory allocated: 6044
Memory reserved: 32258

Update step: 1066. Tokens seen: 279707648
Training loss: 5.492122994019435
Learning rate: 0.00015989999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1079. Tokens seen: 283115520
Training loss: 5.42721781363854
Learning rate: 0.00016184999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1092. Tokens seen: 286523392
Training loss: 5.4227795875989475
Learning rate: 0.0001638
Memory allocated: 6044
Memory reserved: 32258

Update step: 1105. Tokens seen: 289931264
Training loss: 5.384414469966521
Learning rate: 0.00016575
Memory allocated: 6044
Memory reserved: 32258

Update step: 1118. Tokens seen: 293339136
Training loss: 5.374832732173113
Learning rate: 0.0001677
Memory allocated: 6044
Memory reserved: 32258

Update step: 1131. Tokens seen: 296747008
Training loss: 5.389087779017595
Learning rate: 0.00016964999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1144. Tokens seen: 300154880
Training loss: 5.859642607661394
Learning rate: 0.00017159999999999997
Memory allocated: 6044
Memory reserved: 32258

Update step: 1157. Tokens seen: 303562752
Training loss: 5.493673336047393
Learning rate: 0.00017355
Memory allocated: 6044
Memory reserved: 32258
