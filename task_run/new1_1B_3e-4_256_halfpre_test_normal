Starting script
Initializing wandb

pad_token_id: 32000

----------------------------------------
Prepare Model for Int8 Training
----------------------------------------
Model parameters dtype: torch.uint8

Memory allocated before model put to GPU: 0, Unit: MB, same below.
Memory allocated after model put to GPU: 1676
Memory ocupied by model: 1676
The otherwise_dtype is torch.float32, which is used for all tensors not quantized in this optimizer.
Adam-mini found the param block with name: model.embed_tokens.weight torch.Size([32001, 2048])
Adam-mini found the param block with name: model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.0.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.0.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.0.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.0.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.0.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.0.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.0.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.0.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.1.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.1.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.1.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.1.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.1.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.1.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.1.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.1.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.1.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.2.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.2.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.2.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.2.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.2.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.2.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.2.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.2.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.2.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.3.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.3.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.3.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.3.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.3.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.3.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.3.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.3.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.3.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.4.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.4.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.4.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.4.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.4.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.4.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.4.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.4.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.4.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.5.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.5.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.5.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.5.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.5.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.5.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.5.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.5.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.5.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.6.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.6.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.6.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.6.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.6.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.6.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.6.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.6.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.6.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.7.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.7.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.7.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.7.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.7.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.7.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.7.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.7.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.7.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.8.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.8.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.8.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.8.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.8.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.8.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.8.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.8.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.8.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.9.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.9.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.9.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.9.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.9.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.9.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.9.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.9.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.9.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.10.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.10.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.10.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.10.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.10.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.10.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.10.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.10.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.10.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.11.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.11.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.11.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.11.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.11.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.11.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.11.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.11.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.11.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.12.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.12.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.12.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.12.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.12.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.12.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.12.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.12.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.12.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.13.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.13.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.13.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.13.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.13.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.13.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.13.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.13.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.13.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.14.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.14.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.14.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.14.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.14.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.14.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.14.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.14.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.14.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.15.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.15.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.15.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.15.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.15.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.15.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.15.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.15.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.15.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.16.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.16.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.16.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.16.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.16.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.16.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.16.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.16.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.16.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.17.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.17.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.17.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.17.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.17.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.17.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.17.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.17.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.17.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.18.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.18.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.18.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.18.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.18.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.18.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.18.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.18.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.18.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.19.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.19.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.19.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.19.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.19.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.19.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.19.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.19.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.19.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.20.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.20.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.20.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.20.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.20.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.20.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.20.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.20.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.20.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.21.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.21.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.21.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.21.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.21.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.21.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.21.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.21.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.21.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.22.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.22.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.22.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.22.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.22.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.22.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.22.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.22.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.22.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.23.self_attn.q_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.23.self_attn.k_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.23.self_attn.v_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.23.self_attn.o_proj.weight torch.Size([2048, 2048])
Adam-mini found the param block with name: model.layers.23.mlp.gate_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.23.mlp.down_proj.weight torch.Size([2048, 5461])
Adam-mini found the param block with name: model.layers.23.mlp.up_proj.weight torch.Size([5461, 2048])
Adam-mini found the param block with name: model.layers.23.input_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.layers.23.post_attention_layernorm.weight torch.Size([2048])
Adam-mini found the param block with name: model.norm.weight torch.Size([2048])
Adam-mini found the param block with name: lm_head.weight torch.Size([32001, 2048])
Adam-mini found 1 embedding layers, 1 output layers; 48 Querys and Keys;  0 Values;  24 attn_proj;  72 MLPs;
=====>>>  Warning by Adam-mini: No Value found. If you are training Transformers, please check the name of your Value in attention blocks and manually add them to 'self.wv_names' of Adam-mini. You can do this by adding an additional lines of code: optimizer.wv_names.add('the keywords in the  name of your Value' ). 
Memory allocated: 4533
Memory reserved: 30212

Update step: 13. Tokens seen: 3670016
Training loss: 10.611297011375427
Learning rate: 1.9499999999999995e-06
Memory allocated: 6044
Memory reserved: 31256

Update step: 26. Tokens seen: 7077888
Training loss: 9.66221902691401
Learning rate: 3.899999999999999e-06
Memory allocated: 6044
Memory reserved: 31256

Update step: 39. Tokens seen: 10485760
Training loss: 8.809543426220234
Learning rate: 5.85e-06
Memory allocated: 6044
Memory reserved: 31256

Update step: 52. Tokens seen: 13893632
Training loss: 8.435126552214989
Learning rate: 7.799999999999998e-06
Memory allocated: 6044
Memory reserved: 31256

Update step: 65. Tokens seen: 17301504
Training loss: 8.17830333686792
Learning rate: 9.75e-06
Memory allocated: 6044
Memory reserved: 31256

Update step: 78. Tokens seen: 20709376
Training loss: 7.944007333654624
Learning rate: 1.17e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 91. Tokens seen: 24117248
Training loss: 7.701106014160009
Learning rate: 1.3649999999999998e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 104. Tokens seen: 27525120
Training loss: 7.443472180228967
Learning rate: 1.5599999999999996e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 117. Tokens seen: 30932992
Training loss: 7.235956808695426
Learning rate: 1.755e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 130. Tokens seen: 34340864
Training loss: 7.049590350343631
Learning rate: 1.95e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 143. Tokens seen: 37748736
Training loss: 6.920135571406438
Learning rate: 2.1449999999999996e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 156. Tokens seen: 41156608
Training loss: 6.752741360893617
Learning rate: 2.34e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 169. Tokens seen: 44564480
Training loss: 6.627870011788148
Learning rate: 2.535e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 182. Tokens seen: 47972352
Training loss: 6.5755345454582805
Learning rate: 2.7299999999999996e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 195. Tokens seen: 51380224
Training loss: 6.419191080790299
Learning rate: 2.925e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 208. Tokens seen: 54788096
Training loss: 6.332631657329889
Learning rate: 3.119999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 221. Tokens seen: 58195968
Training loss: 6.2753999313482876
Learning rate: 3.315e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 234. Tokens seen: 61603840
Training loss: 6.141264987679628
Learning rate: 3.51e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 247. Tokens seen: 65011712
Training loss: 6.050538499767963
Learning rate: 3.705e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 260. Tokens seen: 68419584
Training loss: 5.973270004758468
Learning rate: 3.9e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 273. Tokens seen: 71827456
Training loss: 5.909095695385566
Learning rate: 4.095e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 286. Tokens seen: 75235328
Training loss: 5.821278604177328
Learning rate: 4.289999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 299. Tokens seen: 78643200
Training loss: 5.770652673565424
Learning rate: 4.484999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 312. Tokens seen: 82051072
Training loss: 5.69986849794021
Learning rate: 4.68e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 325. Tokens seen: 85458944
Training loss: 5.629246371296736
Learning rate: 4.875e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 338. Tokens seen: 88866816
Training loss: 5.602549886474242
Learning rate: 5.07e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 351. Tokens seen: 92274688
Training loss: 5.541288432020408
Learning rate: 5.264999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 364. Tokens seen: 95682560
Training loss: 5.485107810451434
Learning rate: 5.459999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 377. Tokens seen: 99090432
Training loss: 5.501057158295925
Learning rate: 5.654999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 390. Tokens seen: 102498304
Training loss: 5.4724968912509775
Learning rate: 5.85e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 403. Tokens seen: 105906176
Training loss: 5.393362313508987
Learning rate: 6.045e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 416. Tokens seen: 109314048
Training loss: 5.387876707773942
Learning rate: 6.239999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 429. Tokens seen: 112721920
Training loss: 5.31096441241411
Learning rate: 6.434999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 442. Tokens seen: 116129792
Training loss: 5.29970654959862
Learning rate: 6.63e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 455. Tokens seen: 119537664
Training loss: 5.238359645009041
Learning rate: 6.824999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 468. Tokens seen: 122945536
Training loss: 5.252896179373447
Learning rate: 7.02e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 481. Tokens seen: 126353408
Training loss: 5.191952046293479
Learning rate: 7.214999999999999e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 494. Tokens seen: 129761280
Training loss: 5.161104219464155
Learning rate: 7.41e-05
Memory allocated: 6044
Memory reserved: 31256

Update step: 507. Tokens seen: 133169152
Training loss: 5.12274341399853
Learning rate: 7.604999999999999e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 520. Tokens seen: 136577024
Training loss: 5.0866312980651855
Learning rate: 7.8e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 533. Tokens seen: 139984896
Training loss: 5.041125435095567
Learning rate: 7.994999999999999e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 546. Tokens seen: 143392768
Training loss: 5.026739141115775
Learning rate: 8.19e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 559. Tokens seen: 146800640
Training loss: 4.984667091415479
Learning rate: 8.385e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 572. Tokens seen: 150208512
Training loss: 4.9531006927673635
Learning rate: 8.579999999999998e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 585. Tokens seen: 153616384
Training loss: 4.9708236914414625
Learning rate: 8.774999999999999e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 598. Tokens seen: 157024256
Training loss: 4.911005279765679
Learning rate: 8.969999999999998e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 611. Tokens seen: 160432128
Training loss: 4.908710331870959
Learning rate: 9.164999999999999e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 624. Tokens seen: 163840000
Training loss: 4.859914904603591
Learning rate: 9.36e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 637. Tokens seen: 167247872
Training loss: 4.854278650421363
Learning rate: 9.554999999999999e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 650. Tokens seen: 170655744
Training loss: 4.81992788039721
Learning rate: 9.75e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 663. Tokens seen: 174063616
Training loss: 4.823322764956034
Learning rate: 9.944999999999999e-05
Memory allocated: 6044
Memory reserved: 32258

Update step: 676. Tokens seen: 177471488
Training loss: 4.866018522244233
Learning rate: 0.0001014
Memory allocated: 6044
Memory reserved: 32258

Update step: 689. Tokens seen: 180879360
Training loss: 4.7619978630772
Learning rate: 0.00010334999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 702. Tokens seen: 184287232
Training loss: 4.737174712694609
Learning rate: 0.00010529999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 715. Tokens seen: 187695104
Training loss: 4.711820698701418
Learning rate: 0.00010724999999999999
Memory allocated: 6044
Memory reserved: 32258

Update step: 728. Tokens seen: 191102976
Training loss: 4.72957704617427
Learning rate: 0.00010919999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 741. Tokens seen: 194510848
Training loss: 4.696506621745916
Learning rate: 0.00011114999999999999
Memory allocated: 6044
Memory reserved: 32258

Update step: 754. Tokens seen: 197918720
Training loss: 4.645100546570925
Learning rate: 0.00011309999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 767. Tokens seen: 201326592
Training loss: 4.649943017042601
Learning rate: 0.00011504999999999999
Memory allocated: 6044
Memory reserved: 32258

Update step: 780. Tokens seen: 204734464
Training loss: 4.608083780568379
Learning rate: 0.000117
Memory allocated: 6044
Memory reserved: 32258

Update step: 793. Tokens seen: 208142336
Training loss: 4.621473930202997
Learning rate: 0.00011894999999999999
Memory allocated: 6044
Memory reserved: 32258

Update step: 806. Tokens seen: 211550208
Training loss: 4.575300221259777
Learning rate: 0.0001209
Memory allocated: 6044
Memory reserved: 32258

Update step: 819. Tokens seen: 214958080
Training loss: 4.559441113128112
Learning rate: 0.00012284999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 832. Tokens seen: 218365952
Training loss: 4.526734180748463
Learning rate: 0.00012479999999999997
Memory allocated: 6044
Memory reserved: 32258

Update step: 845. Tokens seen: 221773824
Training loss: 4.550010124651285
Learning rate: 0.00012675
Memory allocated: 6044
Memory reserved: 32258

Update step: 858. Tokens seen: 225181696
Training loss: 4.502046684233042
Learning rate: 0.00012869999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 871. Tokens seen: 228589568
Training loss: 4.480197691000425
Learning rate: 0.00013064999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 884. Tokens seen: 231997440
Training loss: 4.453513396474031
Learning rate: 0.0001326
Memory allocated: 6044
Memory reserved: 32258

Update step: 897. Tokens seen: 235405312
Training loss: 4.482105444257076
Learning rate: 0.00013455
Memory allocated: 6044
Memory reserved: 32258

Update step: 910. Tokens seen: 238813184
Training loss: 4.42941458351337
Learning rate: 0.00013649999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 923. Tokens seen: 242221056
Training loss: 4.438299164749109
Learning rate: 0.00013845
Memory allocated: 6044
Memory reserved: 32258

Update step: 936. Tokens seen: 245628928
Training loss: 4.380024876158971
Learning rate: 0.0001404
Memory allocated: 6044
Memory reserved: 32258

Update step: 949. Tokens seen: 249036800
Training loss: 4.4024090978961725
Learning rate: 0.00014235
Memory allocated: 6044
Memory reserved: 32258

Update step: 962. Tokens seen: 252444672
Training loss: 4.365924059198453
Learning rate: 0.00014429999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 975. Tokens seen: 255852544
Training loss: 4.353776969015598
Learning rate: 0.00014624999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 988. Tokens seen: 259260416
Training loss: 4.336669275393853
Learning rate: 0.0001482
Memory allocated: 6044
Memory reserved: 32258

Update step: 1001. Tokens seen: 262668288
Training loss: 4.339144981251313
Learning rate: 0.00015014999999999996
Memory allocated: 6044
Memory reserved: 32258

Update step: 1014. Tokens seen: 266076160
Training loss: 4.310476981676542
Learning rate: 0.00015209999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1027. Tokens seen: 269484032
Training loss: 4.296884801525336
Learning rate: 0.00015404999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1040. Tokens seen: 272891904
Training loss: 4.266994312405586
Learning rate: 0.000156
Memory allocated: 6044
Memory reserved: 32258

Update step: 1053. Tokens seen: 276299776
Training loss: 4.2463564821160755
Learning rate: 0.00015794999999999996
Memory allocated: 6044
Memory reserved: 32258

Update step: 1066. Tokens seen: 279707648
Training loss: 4.255258061564886
Learning rate: 0.00015989999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1079. Tokens seen: 283115520
Training loss: 4.216623200246921
Learning rate: 0.00016184999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1092. Tokens seen: 286523392
Training loss: 4.240990888613921
Learning rate: 0.0001638
Memory allocated: 6044
Memory reserved: 32258

Update step: 1105. Tokens seen: 289931264
Training loss: 4.193808842736941
Learning rate: 0.00016575
Memory allocated: 6044
Memory reserved: 32258

Update step: 1118. Tokens seen: 293339136
Training loss: 4.188255729583593
Learning rate: 0.0001677
Memory allocated: 6044
Memory reserved: 32258

Update step: 1131. Tokens seen: 296747008
Training loss: 4.153700494422362
Learning rate: 0.00016964999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1144. Tokens seen: 300154880
Training loss: 4.1192052112175865
Learning rate: 0.00017159999999999997
Memory allocated: 6044
Memory reserved: 32258

Update step: 1157. Tokens seen: 303562752
Training loss: 4.125903055644953
Learning rate: 0.00017355
Memory allocated: 6044
Memory reserved: 32258

Update step: 1170. Tokens seen: 306970624
Training loss: 4.131099823002632
Learning rate: 0.00017549999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1183. Tokens seen: 310378496
Training loss: 4.076577387177027
Learning rate: 0.00017745
Memory allocated: 6044
Memory reserved: 32258

Update step: 1196. Tokens seen: 313786368
Training loss: 4.084250051814776
Learning rate: 0.00017939999999999997
Memory allocated: 6044
Memory reserved: 32258

Update step: 1209. Tokens seen: 317194240
Training loss: 4.065995474847464
Learning rate: 0.00018135
Memory allocated: 6044
Memory reserved: 32258

Update step: 1222. Tokens seen: 320602112
Training loss: 4.0446529892774725
Learning rate: 0.00018329999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1235. Tokens seen: 324009984
Training loss: 4.024427400758634
Learning rate: 0.00018525
Memory allocated: 6044
Memory reserved: 32258

Update step: 1248. Tokens seen: 327417856
Training loss: 4.005586011478534
Learning rate: 0.0001872
Memory allocated: 6044
Memory reserved: 32258

Update step: 1261. Tokens seen: 330825728
Training loss: 3.9888029946730685
Learning rate: 0.00018914999999999996
Memory allocated: 6044
Memory reserved: 32258

Update step: 1274. Tokens seen: 334233600
Training loss: 3.998159638964213
Learning rate: 0.00019109999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1287. Tokens seen: 337641472
Training loss: 3.9644936254391303
Learning rate: 0.00019304999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1300. Tokens seen: 341049344
Training loss: 3.974474618068108
Learning rate: 0.000195
Memory allocated: 6044
Memory reserved: 32258

Update step: 1313. Tokens seen: 344457216
Training loss: 3.9545558066322255
Learning rate: 0.00019694999999999996
Memory allocated: 6044
Memory reserved: 32258

Update step: 1326. Tokens seen: 347865088
Training loss: 3.91137691701834
Learning rate: 0.00019889999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1339. Tokens seen: 351272960
Training loss: 3.930008003918024
Learning rate: 0.00020084999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1352. Tokens seen: 354680832
Training loss: 3.903292330411764
Learning rate: 0.0002028
Memory allocated: 6044
Memory reserved: 32258

Update step: 1365. Tokens seen: 358088704
Training loss: 3.8742789809520426
Learning rate: 0.00020475
Memory allocated: 6044
Memory reserved: 32258

Update step: 1378. Tokens seen: 361496576
Training loss: 3.8782849810444393
Learning rate: 0.00020669999999999996
Memory allocated: 6044
Memory reserved: 32258

Update step: 1391. Tokens seen: 364904448
Training loss: 3.9029547050595284
Learning rate: 0.00020864999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1404. Tokens seen: 368312320
Training loss: 3.8978169904305386
Learning rate: 0.00021059999999999997
Memory allocated: 6044
Memory reserved: 32258

Update step: 1417. Tokens seen: 371720192
Training loss: 3.836243812281352
Learning rate: 0.00021255
Memory allocated: 6044
Memory reserved: 32258

Update step: 1430. Tokens seen: 375128064
Training loss: 3.80179285716552
Learning rate: 0.00021449999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1443. Tokens seen: 378535936
Training loss: 3.8068894987496047
Learning rate: 0.00021645
Memory allocated: 6044
Memory reserved: 32258

Update step: 1456. Tokens seen: 381943808
Training loss: 3.831499375976049
Learning rate: 0.00021839999999999997
Memory allocated: 6044
Memory reserved: 32258

Update step: 1469. Tokens seen: 385351680
Training loss: 3.8056235622901182
Learning rate: 0.00022035
Memory allocated: 6044
Memory reserved: 32258

Update step: 1482. Tokens seen: 388759552
Training loss: 3.7736451293413458
Learning rate: 0.00022229999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1495. Tokens seen: 392167424
Training loss: 3.778945000698933
Learning rate: 0.00022425
Memory allocated: 6044
Memory reserved: 32258

Update step: 1508. Tokens seen: 395575296
Training loss: 3.775536231123484
Learning rate: 0.00022619999999999997
Memory allocated: 6044
Memory reserved: 32258

Update step: 1521. Tokens seen: 398983168
Training loss: 3.802853467945869
Learning rate: 0.00022814999999999996
Memory allocated: 6044
Memory reserved: 32258

Update step: 1534. Tokens seen: 402391040
Training loss: 3.7939436699335394
Learning rate: 0.00023009999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1547. Tokens seen: 405798912
Training loss: 3.7819543968026457
Learning rate: 0.00023204999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1560. Tokens seen: 409206784
Training loss: 3.764106346724125
Learning rate: 0.000234
Memory allocated: 6044
Memory reserved: 32258

Update step: 1573. Tokens seen: 412614656
Training loss: 3.7454295164117446
Learning rate: 0.00023594999999999996
Memory allocated: 6044
Memory reserved: 32258

Update step: 1586. Tokens seen: 416022528
Training loss: 3.742358911495942
Learning rate: 0.00023789999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1599. Tokens seen: 419430400
Training loss: 3.721691507846117
Learning rate: 0.00023984999999999998
Memory allocated: 6044
Memory reserved: 32258

Update step: 1612. Tokens seen: 422838272
Training loss: 3.760163915845064
Learning rate: 0.0002418
Memory allocated: 6044
Memory reserved: 32258

Update step: 1625. Tokens seen: 426246144
Training loss: 3.7228446270410833
Learning rate: 0.00024375
Memory allocated: 6044
Memory reserved: 32258

Update step: 1638. Tokens seen: 429654016
Training loss: 3.7095482962635846
Learning rate: 0.00024569999999999995
Memory allocated: 6044
Memory reserved: 32258

Update step: 1651. Tokens seen: 433061888
Training loss: 3.6939106391599545
Learning rate: 0.00024765
Memory allocated: 6044
Memory reserved: 32258

Update step: 1664. Tokens seen: 436469760
Training loss: 3.69299596032271
Learning rate: 0.00024959999999999994
Memory allocated: 6044
Memory reserved: 32258

Update step: 1677. Tokens seen: 439877632
Training loss: 3.662419298520455
Learning rate: 0.00025154999999999996
Memory allocated: 6044
Memory reserved: 32258

Update step: 1690. Tokens seen: 443285504
Training loss: 3.689477120454495
Learning rate: 0.0002535
Memory allocated: 6044
Memory reserved: 32258

Update step: 1703. Tokens seen: 446693376
Training loss: 3.682285516881026
Learning rate: 0.00025545
Memory allocated: 6044
Memory reserved: 32258

Update step: 1716. Tokens seen: 450101248
Training loss: 3.66070054184932
Learning rate: 0.00025739999999999997
Memory allocated: 6044
Memory reserved: 32258

Update step: 1729. Tokens seen: 453509120
Training loss: 3.664679939357134
Learning rate: 0.00025935
Memory allocated: 6044
Memory reserved: 32258

Update step: 1742. Tokens seen: 456916992
Training loss: 3.6245602340652394
Learning rate: 0.00026129999999999995
Memory allocated: 6044
Memory reserved: 32258

Update step: 1755. Tokens seen: 460324864
Training loss: 3.6232302091442623
Learning rate: 0.00026325
Memory allocated: 6044
Memory reserved: 32258
